---
title: Q* (Q-Star)
description: Self-improving, self-learning AI step
icon: "brain-circuit"
---

The **Q*** (Q-Star) step is a self-improving, self-learning step that learns from agent modifications to continuously improve output.

## Overview

Q* represents an advanced AI capability that learns from changes made to SmartAgent configurations and automatically refines responses over time without requiring manual retraining.

## Key Features

### Self-Learning
Q* observes modifications made to SmartAgent workflows and learns from these changes to improve future responses.

### Continuous Improvement
Unlike static models, Q* evolves over time as it learns from:
- Agent edits to AI-generated responses
- Workflow modifications
- Pattern recognition from conversation outcomes
- Feedback signals

### No Manual Retraining Required
Traditional AI systems need explicit retraining cycles. Q* learns automatically from:
- Configuration changes
- Step modifications
- Response adjustments
- Workflow updates

## How It Works

1. **Baseline Performance**: Q* starts with initial AI capabilities
2. **Monitor Changes**: Observes agent modifications and workflow updates
3. **Learn Patterns**: Identifies patterns in what changes improve outcomes
4. **Apply Learning**: Automatically incorporates learned improvements
5. **Continuous Evolution**: Repeats this cycle indefinitely

## Use Cases

### Adaptive Response Generation
Q* adapts to evolving communication styles and customer preferences without manual updates.

### Pattern Recognition
Identifies successful conversation patterns and applies them to similar scenarios.

### Workflow Optimization
Learns which workflow structures produce the best outcomes and suggests improvements.

### Contextual Understanding
Improves understanding of domain-specific language and terminology over time.

## Benefits

**Automatic Improvement**
- No need for manual model retraining
- Continuous enhancement of performance
- Adapts to changing requirements

**Reduced Maintenance**
- Less manual intervention required
- Self-optimizing workflows
- Lower operational overhead

**Performance Gains**
- Better outcomes over time
- Faster adaptation to new scenarios
- Improved accuracy and relevance

## Configuration

### Learning Rate
Control how quickly Q* adapts to changes (when available).

### Learning Scope
Define what types of changes Q* should learn from.

### Feedback Integration
Configure how feedback signals influence learning.

## Best Practices

1. **Monitor Performance** - Track how Q* improves over time
2. **Provide Clear Feedback** - The more structured feedback, the better Q* learns
3. **Start Simple** - Begin with straightforward use cases
4. **Review Periodically** - Ensure learning aligns with business goals
5. **Document Changes** - Keep track of what modifications drive improvements

## Comparison to Traditional LLM Steps

| Traditional LLM Step | Q* Step |
|---|---|
| Static model behavior | Adaptive learning |
| Requires retraining | Self-improving |
| Fixed knowledge | Evolving knowledge |
| Manual optimization | Automatic optimization |

## Considerations

**Data Privacy**
Ensure that Q*'s learning doesn't incorporate sensitive information inappropriately.

**Monitoring**
Regularly review Q*'s learned behaviors to ensure alignment with business objectives.

**Validation**
Test Q*'s outputs, especially after significant learning periods.

## Related Pages

- [LLM Step](/platform/steps/llm-step) - Traditional AI-driven response generation
- [SmartAgent Builder](/platform/smartAgentBuilder) - Configure Q* within workflows
- [Testing Suite](/platform/evals) - Validate Q* performance improvements